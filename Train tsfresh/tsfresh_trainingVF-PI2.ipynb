{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tsfresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster1_path = 'cluster1.csv' \n",
    "cluester1 = pd.read_csv(cluster1_path)\n",
    "cluester1.drop(columns = ['Unnamed: 0'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluester1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_label = {\n",
    "            'act_1' : 0,\n",
    "            'act_2': 1\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluester1['label']= cluester1['label'].map(dict_label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluester1[\"date\"] = pd.to_datetime(cluester1[\"date\"],format = '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluester1.sort_values(['pair','date'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluester1['pair_index_ant'] = cluester1.groupby('pair')['pair_index'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluester1['retorno'] = cluester1['pair_index']/cluester1['pair_index_ant']-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cluester1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluester1_x = cluester1[['pair','date','retorno']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import make_forecasting_frame\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.utilities.dataframe_functions import roll_time_series\n",
    "from tsfresh import select_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_1 = cluester1_x[cluester1_x['pair']=='EWH US Equity_EWY US Equity'].dropna().drop(columns = ['pair']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsfresh_feautures = pd.DataFrame()\n",
    "for k, v in cluester1_x.groupby('pair'):\n",
    "    aux = cluester1_x[cluester1_x['pair']==k].dropna().drop(columns = ['pair']).copy()\n",
    "    aux.set_index('date', inplace = True)\n",
    "    aux_2 = aux['retorno'].copy()\n",
    "    df_shift, y = make_forecasting_frame(aux_2, kind=\"retorno\", max_timeshift=50, rolling_direction=1)\n",
    "    aux_3 = extract_features(df_shift, column_id=\"id\", column_sort=\"time\", column_value=\"value\", impute_function=impute, \n",
    "                     show_warnings=False)\n",
    "    aux_3['pair'] = k\n",
    "    aux_3.reset_index(inplace = True)\n",
    "    aux_3.rename(columns = {'id':'date'},inplace = True)\n",
    "#    aux_3['date']=aux_3['date'].shift(1) # para mover las caracteristicas un dia atras.\n",
    "    tsfresh_feautures = pd.concat([tsfresh_feautures,aux_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_shift\n",
    "#tsfresh_feautures.rename(columns = {'id':'date'},inplace = True)\n",
    "tsfresh_feautures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluester1_all = cluester1.merge(tsfresh_feautures, how = 'left', on = ['date','pair'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluester1_all_no = cluester1_all.drop(columns = ['pair_index']).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "#%%\n",
    "\n",
    "models = {\n",
    "            'linear' : {\n",
    "                    'mod' : LogisticRegression(),\n",
    "                    'par' : {'solver' : ('newton-cg', 'lbfgs'),\n",
    "                                 'class_weight' : (None, 'balanced')}\n",
    "                    },                     \n",
    "            'gradient' : {\n",
    "                    'mod' : GradientBoostingClassifier(warm_start = True),\n",
    "                    'par' : {'loss' : ('deviance', 'exponential'),\n",
    "                             'max_depth' : [3, 4, 5, 6, 7]}\n",
    "                        },\n",
    "            'SVM' : {\n",
    "                    'mod' : SVC(gamma='auto',probability=True),\n",
    "                    'par' : {'kernel':('linear', 'poly', 'rbf', 'sigmoid'), 'C':[1, 10]}\n",
    "                    },\n",
    "            'Random_Forest' : {\n",
    "                    'mod' : RandomForestClassifier(random_state=42),\n",
    "                    'par' : {    'n_estimators': [100, 300],'max_features': ['auto', 'sqrt', 'log2'],'max_depth' : [4,5,6],\n",
    "                    'criterion' :['gini', 'entropy']}\n",
    "                    },\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid(now_date, n_proc, os_X_tt, os_Y_tt, \n",
    "         models, score = 'roc_auc', cv = 5):\n",
    "    # Gridsearch\n",
    "    \n",
    "    for name in models:\n",
    "        print('*'*80)\n",
    "        print(\"Model: \" + name)\n",
    "        t_beg = time.time()\n",
    "\n",
    "        pipeline = Pipeline([('scaler', StandardScaler()), (name,  models[name]['mod'])])          \n",
    "        parameters = {}          \n",
    "        for par in models[name]['par']:\n",
    "            aux = name + '__' +  par\n",
    "            parameters[aux] = models[name]['par'][par]    \n",
    "        aux = GridSearchCV(pipeline, parameters, n_jobs = n_proc,\\\n",
    "                          scoring = score, verbose=2, cv = cv)\n",
    "        aux.fit(os_X_tt, os_Y_tt)\n",
    "        models[name]['bestModel'] = aux.best_estimator_\n",
    "        models[name]['mae'] = aux.best_score_\n",
    "        models[name]['cols_order'] = os_X_tt.columns.values\n",
    "        selection_time = time.time() - t_beg\n",
    "\n",
    "        models[name]['selection_time'] = selection_time\n",
    "\n",
    "        sample_f_path = f'{name}_{now_date.strftime(\"%Y%m%d-%H%M\")}.sav'\n",
    "\n",
    "        print(f\"Saving model at {sample_f_path}\")    \n",
    "        joblib.dump(models[name]['bestModel'], sample_f_path)\n",
    "\n",
    "        print(f\"El tiempo de seleccion fue: {selection_time:0.3f} s\")\n",
    "        print(f\"El error f_score de la familia {name} es: {models[name]['mae']:0.3f}\")\n",
    "        print('*'*80)\n",
    "\n",
    "    mod_name = None\n",
    "    best_mae = -np.inf\n",
    "    for name in models:\n",
    "        if models[name]['mae'] > best_mae:\n",
    "            mod_name = name\n",
    "            best_mae = models[name]['mae']\n",
    "\n",
    "    print(f\"best model: \" + mod_name + \" with an error of: \" + str(best_mae))\n",
    "\n",
    "    return models,mod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def all_years(fecha_ini, fecha_fin, intervalo, X, y):\n",
    "def all_years(fecha_ini, fecha_fin, intervalo,cluester1_all_no,label = 'label'):\n",
    "    #Completar los datos\n",
    "    minimo = fecha_ini\n",
    "    maximo= fecha_fin\n",
    "    resultados_final = pd.DataFrame()\n",
    "    #ix = pd.DatetimeIndex(start=datetime(diasconsulta.year, diasconsulta.month, diasconsulta.day, 0,0,0), end=datetime(diasconsulta.year, diasconsulta.month, diasconsulta.day,23,0,0), freq='H')\n",
    "    ix = pd.date_range(start=dt.datetime(minimo.year, minimo.month, minimo.day,0,0,0), end=dt.datetime(maximo.year, maximo.month, maximo.day,0,0,0), freq=intervalo)\n",
    "    for year in ix:\n",
    "        print('*************')\n",
    "        inicio = dt.datetime(year.year-5, 1, 1)\n",
    "        fin = year\n",
    "        \n",
    "        fin2 = dt.datetime(year.year+2, 1, 1)\n",
    "        print(inicio)\n",
    "        print(fin)\n",
    "        print(fin2)\n",
    "        train = cluester1_all_no[(cluester1_all_no['date']>=inicio)&(cluester1_all_no['date']<=fin)]\n",
    "        test = cluester1_all_no[(cluester1_all_no['date']>fin)&(cluester1_all_no['date']<fin2)]\n",
    "        train.set_index(['pair','date'], inplace = True)\n",
    "        test.set_index(['pair','date'], inplace = True)\n",
    "        X_train = train.drop(columns = [label])\n",
    "        Y_train = train[label]\n",
    "        X_train_selected = select_features(X_train, Y_train)\n",
    "        X_test = test[X_train_selected.columns].copy()\n",
    "        Y_test = test[label]\n",
    "        now_date = dt.date.today()\n",
    "        n_proc = 3\n",
    "        models_best,model_name = grid(now_date, n_proc, X_train_selected, Y_train, models, score = 'roc_auc', cv = 5)\n",
    "        y_pred =  models_best[model_name]['bestModel'].predict_proba(X_test)\n",
    "        X_test_tot = X_test.copy()\n",
    "        X_test_tot['Pred'] = y_pred[:,1]\n",
    "        X_test_tot['Real'] = test[label]\n",
    "        resultados_final = pd.concat([resultados_final,X_test_tot[['Pred','Real']]])\n",
    "    return resultados_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fecha_ini = dt.datetime(2014, 1, 1)\n",
    "fecha_fin = dt.datetime(2019, 1, 1)\n",
    "intervalo = 'Y'\n",
    "total = all_years(fecha_ini, fecha_fin, intervalo,cluester1_all_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total['Real'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.to_csv('cluesterAll_final_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in total.groupby('pair'):\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Portafolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster1=['MCHI US Equity','EWH US Equity','EWY US Equity'] #asia   \n",
    "cluster2=['EWQ US Equity','EWG US Equity','EWU US Equity'] #europa   \n",
    "cluster3=['XLI US Equity','XLB US Equity','XLF US Equity','XLE US Equity'] #USA sectores1\n",
    "cluster4=['INDA US Equity','THD US Equity','EWM US Equity'] #asia superemergente\n",
    "cluster5=['SPY US Equity','XLK US Equity','XLY US Equity'] #USA sectores2\n",
    "clusterAll=['SPY US Equity','INDA US Equity','EWQ US Equity','XLF US Equity','MCHI US Equity'] #Uno de todos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster=clusterAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfprobv=total.drop(columns = 'Real')\n",
    "dfprobv=dfprobv.unstack().T\n",
    "dfprobv.reset_index(inplace = True)\n",
    "dfprobv.drop(['level_0'],axis=1,inplace = True)\n",
    "dfprobv.set_index('date',inplace = True)\n",
    "dfprobv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "comb = combinations(list(range(len(cluster))), 2)\n",
    "etfs=[]\n",
    "for par in list(comb): \n",
    "    etfs.append(cluster[par[0]]+'_'+cluster[par[1]])\n",
    "\n",
    "    print(etfs)\n",
    "\n",
    "dfprobv = dfprobv[etfs].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votosv=pd.DataFrame(0, index=dfprobv.index.tolist(), columns=cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb = combinations(list(range(len(cluster))), 2)\n",
    "\n",
    "j=0\n",
    "for par in list(comb):\n",
    "    for i in range(len(votosv)):\n",
    "       \n",
    "        if dfprobv.iloc[i,j]>0.6:\n",
    "            votosv.iloc[i].loc[cluster[par[0]]]+=2\n",
    "            \n",
    "        elif dfprobv.iloc[i,j]<0.4:\n",
    "            votosv.iloc[i].loc[cluster[par[1]]]+=2\n",
    "            \n",
    "        else: \n",
    "            votosv.iloc[i].loc[cluster[par[0]]]+=1\n",
    "            votosv.iloc[i].loc[cluster[par[1]]]+=1\n",
    "    j+=1\n",
    "    \n",
    "votosv= (votosv.T / votosv.T.sum()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### precios etfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'all_info.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allinfotypes = {\"date\":str,\n",
    "                \"PX_LAST\":float,\n",
    "                \"PX_OPEN\":float,\n",
    "                \"PX_HIGH\":float,\n",
    "                \"PX_LOW\":float,\n",
    "                \"PX_VOLUME\":float,\n",
    "                \"TOT_RETURN_INDEX_NET_DVDS\":float,\n",
    "                \"etf\":str}\n",
    "rawdata = pd.read_csv(file_name,sep =\",\",encoding = 'UTF-8', dtype = allinfotypes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata[\"date\"] = pd.to_datetime(rawdata[\"date\"],format = '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_relevant = [\"date\",\"TOT_RETURN_INDEX_NET_DVDS\",\"etf\"]\n",
    "datafil = rawdata[columns_relevant].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafilind = datafil.set_index(['date','etf']).unstack()\n",
    "cols = [f\"{l1}\" for (l0, l1) in datafilind.columns]\n",
    "datafilind.columns = cols\n",
    "datafilind.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#votosv.data_to_prepros2[data_to_prepros2.index.dayofweek==2]\n",
    "#votosv[votosv.index.dayofweek==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_prepros2 = datafilind[['date']+cluster].copy()\n",
    "data_to_prepros2.set_index('date',inplace = True)\n",
    "data_to_prepros2=data_to_prepros2[data_to_prepros2.index.dayofweek==2] #miercoles\n",
    "votosv=votosv[votosv.index.dayofweek==2] #miercoles\n",
    "\n",
    "for i in cluster:\n",
    "    name_col = i +'_rt1d'\n",
    "    data_to_prepros2[name_col] =data_to_prepros2[[i]]/data_to_prepros2[[i]].shift(periods=+1) - 1 #validar orden\n",
    "\n",
    "data_to_prepros2=pd.merge(data_to_prepros2, votosv, left_index=True, right_index=True)\n",
    "columns=data_to_prepros2.columns.tolist()\n",
    "data_to_prepros2=data_to_prepros2[columns[:len(columns)-len(cluster)]]\n",
    "data_to_prepros2['strategy']=pd.DataFrame(data_to_prepros2[data_to_prepros2.columns[len(cluster):len(cluster)*2]].values*votosv.shift(1).values).T.sum().tolist() #shift para multiplicar portafolio por retornos del proximo dia\n",
    "data_to_prepros2['benchmark']=pd.DataFrame(data_to_prepros2[data_to_prepros2.columns[len(cluster):len(cluster)*2]].values*(1/len(cluster))).T.sum().tolist()\n",
    "data_to_prepros2['strategy']=(data_to_prepros2[['strategy']]+1)\n",
    "data_to_prepros2['benchmark']=(data_to_prepros2[['benchmark']]+1)\n",
    "data_to_prepros2.iloc[0].loc['strategy']=data_to_prepros2.iloc[0].loc['benchmark']\n",
    "#data_to_prepros2['strategy']=data_to_prepros2[['strategy']]*data_to_prepros2[['strategy']].shift(periods=+1)\n",
    "#data_to_prepros2['benchmark']=data_to_prepros2[['benchmark']]*data_to_prepros2[['benchmark']].shift(periods=+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_to_prepros2)):\n",
    "    if i!=0:\n",
    "        data_to_prepros2.iloc[i].loc['strategy']=data_to_prepros2.iloc[i].loc['strategy']*data_to_prepros2.iloc[i-1].loc['strategy']\n",
    "        data_to_prepros2.iloc[i].loc['benchmark']=data_to_prepros2.iloc[i].loc['benchmark']*data_to_prepros2.iloc[i-1].loc['benchmark']\n",
    "data_to_prepros2.head()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'date':['2014-12-31'],'strategy': [1000],'benchmark': [1000]})\n",
    "df1[\"date\"] = pd.to_datetime(df1[\"date\"],format = '%Y-%m-%d')\n",
    "df1.set_index('date',inplace = True)\n",
    "df2=pd.DataFrame([data_to_prepros2['strategy']*1000,data_to_prepros2['benchmark']*1000]).T\n",
    "#df2.drop(df2.index[0],inplace = True)\n",
    "result=pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "sns.set(style='darkgrid')\n",
    "sns.lineplot(x=result.index.tolist(),y=result[result.columns[0]],label=result.columns[0])\n",
    "sns.lineplot(x=result.index.tolist(),y=result[result.columns[1]],label=result.columns[1])\n",
    "plt.ylabel(\"price\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#votosv.to_csv('votosc1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelzona=['AAXJ US Equity']\n",
    "data_to_prepros3 = pd.DataFrame(datafilind[['date']+labelzona])\n",
    "data_to_prepros3.set_index('date',inplace = True)\n",
    "\n",
    "data_to_prepros3['rt1d'] =data_to_prepros3[labelzona]/data_to_prepros3[labelzona].shift(periods=+1) #validar orden\n",
    "\n",
    "data_to_prepros3=pd.merge(data_to_prepros3, votosv, left_index=True, right_index=True)\n",
    "columns=data_to_prepros3.columns.tolist()\n",
    "data_to_prepros3=data_to_prepros3.drop(columns=votosv.columns.tolist())\n",
    "#data_to_prepros2['strategy']=data_to_prepros2[['strategy']]*data_to_prepros2[['strategy']].shift(periods=+1)\n",
    "#data_to_prepros2['benchmark']=data_to_prepros2[['benchmark']]*data_to_prepros2[['benchmark']].shift(periods=+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_to_prepros3)):\n",
    "    if i!=0:\n",
    "        data_to_prepros3.iloc[i].loc['rt1d']=data_to_prepros3.iloc[i].loc['rt1d']*data_to_prepros3.iloc[i-1].loc['rt1d']\n",
    "data_to_prepros3.head()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({'date':['2014-12-31'],'rt1d': [1000]})\n",
    "df3[\"date\"] = pd.to_datetime(df3[\"date\"],format = '%Y-%m-%d')\n",
    "df3.set_index('date',inplace = True)\n",
    "df4=pd.DataFrame([data_to_prepros3['rt1d']*1000]).T\n",
    "#df2.drop(df2.index[0],inplace = True)\n",
    "result2=pd.concat([df3, df4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "sns.set(style='darkgrid')\n",
    "sns.lineplot(x=result.index.tolist(),y=result[result.columns[0]],label=result.columns[0])\n",
    "sns.lineplot(x=result.index.tolist(),y=result[result.columns[1]],label=result.columns[1])\n",
    "sns.lineplot(x=result.index.tolist(),y=result2[result2.columns[0]],label=labelzona[0])\n",
    "plt.ylabel(\"price\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
